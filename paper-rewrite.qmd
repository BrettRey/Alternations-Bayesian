---
title: "When to Say *that*"
subtitle: "Subordinator Realization as Linguistic Knowledge"
author: "TBD"
date: "last-modified"
format: html
bibliography: references.bib
citeproc: true
abstract: |
  A four-letter word is doing a lot of work in English. Speakers omit *that*
  almost 90% of the time after *think*, but writers keep it in over 40% of
  academic content clauses. That isn't free variation: it's a learned
  probability distribution tied to verbs, registers, and processing load.
  This paper argues that these probabilities are part of linguistic
  knowledge&nbsp;-- systematic, learnable, and predictively robust. They are
  causally entrenched and projectible: they recur across speakers and settings
  in ways that support reliable prediction and explanation. Using 302,522
  content clauses from the OANC and a Bayesian multilevel model with partial
  pooling over matrix verbs, I show that the model predicts held-out register
  rates within a few percentage points. Out-of-sample prediction provides the
  evidential bridge from descriptive pattern to linguistic knowledge. English
  linguistic knowledge includes not just what's allowed, but how likely speakers are to
  choose among well-formed options.
---

## Introduction

A four-letter word separates these two sentences:

a. *I think that she left early.*
b. *I think she left early.*

These sentences are propositionally near-equivalent&nbsp;-- close enough that speakers regularly choose between them under the same communicative circumstances. Yet they aren't interchangeable. After *think*, speakers in conversation omit *that* nearly 90% of the time. After *demonstrate*, writers in academic prose keep it over 80% of the time. This variation isn't noise: it's learned, verb-specific, register-sensitive, and predictively stable.

**The central claim of this paper:** English linguistic knowledge includes not just what's allowed, but how likely speakers are to choose among well-formed options. Alternation probabilities are linguistic knowledge.

Specifically, they are **morphosyntactic knowledge**. I use "morphosyntactic" rather than "grammatical" to avoid ambiguity. "Grammatical" sometimes means "well-formed" (acceptability), sometimes "morphosyntactic" (as opposed to phonological or lexical), and sometimes "part of linguistic competence." The claim here is that probabilistic preferences among well-formed structural alternatives are morphosyntactic knowledge -- systematic, shared, and predictive -- even though violating them doesn't trigger sharp unacceptability.

The rest of the paper develops this claim. I first situate *that*-omission in *CGEL* and prior work, then model OANC data with a Bayesian multilevel logistic regression and validate out of sample.

## Subordinator realization in English

### The *CGEL* framework

Following *The Cambridge Grammar of the English Language* [@HuddlestonPullum2002],
I use the term [subordinator]{.smallcaps} for the word *that* when it introduces
a finite content clause. *CGEL* distinguishes [*that*-content clauses]{.smallcaps}
(with an overt subordinator) from [bare content clauses]{.smallcaps} (without).
Both function as complements of verbs (*think*, *say*, *demonstrate*), adjectives (*sure*,
*aware*), and nouns (*fact*, *idea*). (The generative tradition calls *that* a
"complementizer," but the terminological difference is immaterial to the present
analysis.) The key point: *CGEL* describes the alternation but does not quantify
it. That's what the present analysis supplies.
report don't depend on it.

*CGEL* notes that *that* is "often optional" in content clauses but doesn't
systematically characterize the conditioning factors. This is typical of
reference grammars: they describe what's possible, not what's probable. The
present analysis takes the next step, asking what determines the probability of
each variant.

### Functions of content clauses

Content clauses serve several grammatical functions, and the availability of the
*that* ~ bare alternation differs across them.

As [subjects]{.smallcaps}, content clauses strongly prefer overt *that*. Compare
*That she left surprised everyone* with the marginal *\*She left surprised
everyone*. Subject content clauses don't exhibit the robust variation we see elsewhere.

As [complements]{.smallcaps} of verbs, adjectives, and nouns, the alternation is
productive. Both *I think that she left* and *I think she left* are fully
acceptable, and the choice between them is conditioned by the factors discussed
below. This is where the action is.

In [extraposed]{.smallcaps} position, content clauses also show variation, though the patterns differ from non-extraposed complements. *It's clear that she left* and *It's clear she left* are both acceptable. As detailed in the Data section, extraposed constructions generally show higher rates of overt *that* than their in-situ counterparts.

The present analysis focuses on complement content clauses (both *in situ* and extraposed)&nbsp;-- the position where variation is most productive. Subject clauses, where *that* is near-obligatory, are excluded. This scoping reflects the fact that grammar has two components: categorical constraints (subjects require *that*) and probabilistic preferences (complements vary). Both are linguistic knowledge. The categorical constraints aren't in dispute. The probabilistic preferences are less understood&nbsp;-- and that's what we're investigating. The rationale for this scoping decision is discussed further under "Envelope of Variation" in the Data section.

### Conditioning factors from prior research

Research on *that*-omission has typically identified four main classes of conditioning factors.

First, **lexical identity** is perhaps the strongest predictor. Individual matrix verbs differ sharply in their subordinator preferences. Classic studies have long noted that high-frequency "cognitive" verbs like *think*, *know*, *guess*, and *say* strongly favour bare clauses, often appearing in formulae that function essentially as epistemic parentheticals [@ThompsonMulac1991; @Tagliamonte2006]. In contrast, more formal or factive verbs like *prove*, *demonstrate*, and *ensure* typically require or strongly favour overt *that* [@BiberEtAl1999]. This lexical specificity suggests that speakers learn verb-by-verb probabilistic patterns, a view consistent with constructionist approaches to argument structure [@Goldberg2006].

Second, **register** plays a major role. Quantitative corpus studies consistently find that spoken English uses bare clauses far more frequently than written English [@BiberEtAl1999]. Within written registers, informal genres (like fiction or personal letters) show higher rates of omission than academic prose or press reportage. These patterns likely reflect a combination of formality, planning time, and genre-specific conventions.

Third, **complexity** affects the choice. Under the *Complexity Principle* [@Rohdenburg1996], more explicit grammatical options (like overt *that*) are preferred in cognitively complex environments. Empirical work supports this: longer content clauses favour overt *that*, as do clauses separated from their matrix verb by intervening material. The subordinator is thought to function as a parsing cue, signalling a clause boundary and facilitating processing in difficult contexts [@BresnanFord2010].

Fourth, **information-theoretic** accounts invoke surprisal. The *Uniform Information Density* (UID) hypothesis predicts that speakers manage information flow by inserting optional function words when the onset of the embedded clause is unpredictable [@LevyJaeger2007; @Jaeger2010; @Levy2008]. If the content clause is highly predictable given the matrix verb (e.g., after *I think*), *that* is redundant and likely omitted. If the clause is unexpected, *that* is inserted to smooth the information profile.

Most prior work uses significance testing to identify these effects. A typical
finding is that some predictor shows a "significant effect" (*p* < .05), but
this framing has limitations. It tells us that an effect isn't zero, not how
large it is or how confident we should be. And it treats each predictor in
isolation, missing the interactions among lexical, register, and processing
factors.

## Alternation probabilities as linguistic knowledge

I propose that the probability of choosing *that* over a bare clause isn't
performance noise or random variation. It's part of speakers' linguistic
competence&nbsp;-- knowledge they acquire from usage and carry with them as part of
knowing English.

This claim has three components. First, the patterns are [systematic]{.smallcaps}:
they're conditioned by identifiable linguistic factors (verb, register, length)
in consistent ways. Second, they're [learnable]{.smallcaps}: the conditioning
factors are available in the input, and speakers converge on similar
probabilities through exposure. Third, they're [predictive]{.smallcaps}: a model
trained on one dataset generalizes to held-out data, showing that the patterns
capture something stable rather than fitting noise.

What makes this "morphosyntactic knowledge" rather than purely pragmatic or stylistic variation? While the
distinction is not sharp, I argue that the alternation is
morphosyntactic insofar as it is *causally entrenched* and *projectible*: it recurs
across speakers and settings in ways that support reliable prediction and
explanation, and it is sensitive to manipulable variables (register/genre
practices, clause complexity, lemma-specific routines). The question isn't
whether to draw a line between morphosyntax and pragmatics, but whether this
phenomenon belongs to a stable node in the causal network that constitutes
linguistic knowledge.

Out-of-sample prediction provides the evidential bridge. If a model trained on
one portion of the data successfully predicts held-out data&nbsp;-- across registers,
across verbs&nbsp;-- then the patterns it captures are projectible in the sense that
matters for scientific generalization. The alternation isn't an artefact of
corpus quirks or individual stylistic whims; it's a regularity that supports
inference to new cases.

Usage-based approaches to grammar have long emphasized that probabilistic
patterns can be grammaticalized&nbsp;-- internalized as part of the language system
rather than computed on the fly [@Tagliamonte2006; @BresnanFord2010; @Goldberg2006].
The present analysis provides quantitative evidence for this view.

### A minimal causal model

What generates the observed patterns? The conditioning factors identified in
prior research suggest a causal structure, even if the details remain
incomplete. At minimum, three classes of causal factors appear to be at work:

1. **Lemma-level entrenchment**: Individual verbs develop baseline omission rates
   through frequency and formulaicity. High-frequency cognitive verbs like *think*
   and *know* occur in routinized epistemic constructions that favour bare clauses;
   this preference becomes entrenched through repetition.

2. **Genre institutions**: Written registers, especially academic prose, are
   subject to editing norms, pedagogical intervention, and publication practices
   that favour explicit marking. These institutional pressures shift the register
   intercept toward overt *that*.

3. **Production and comprehension constraints**: Longer clauses and structurally
   complex environments favour the more explicit variant, consistent with *that*
   functioning as a parsing cue. Any register-by-length interaction reflects
   differential weighting of these cues across production contexts.

This model is deliberately minimal. It doesn't commit to a full mechanistic
account of online processing or acquisition. But it makes the explanatory
structure explicit enough that a critic can identify what would count as
disconfirming evidence: if verb identity showed no effect, or if register
differences disappeared after controlling for verb frequency, or if longer
clauses *disfavoured* *that*, the model would be in trouble.

The theoretical stake is whether grammar is fundamentally categorical (all-or-
nothing rules) or fundamentally gradient (degrees and probabilities). I'm not
claiming that categorical rules don't exist. But I am claiming that alternation
probabilities belong to the same explanatory domain as agreement patterns or
word order rules. They're facts about English that native speakers know [@Bresnan2007].

## A gentle introduction to Bayesian modeling

Many linguists are unfamiliar with Bayesian statistics. This section explains
the approach in plain language, emphasizing what it offers for linguistic
questions. Technical details appear in the Appendix. For broad introductions tailored to social science, see @McElreath2020 and @GelmanHill2007.

### Why Bayesian?

Rather than a binary *p* < .05 decision, a Bayesian model yields uncertainty intervals for effect sizes and shrinks sparse-verb estimates via partial pooling. This matters for alternations: common verbs like *think* provide ample data, but rare verbs like *allege* appear only a few times. A naive model estimates rare verbs unreliably. Partial pooling assumes all verbs come from a common population and "shrinks" extreme estimates toward the overall mean&nbsp;-- a principled way to regularize without throwing out data.

### Understanding the output

To interpret the model's output, two concepts are helpful.

First, effects are reported in [log-odds]{.smallcaps} (logits), the standard scale for logistic regression.
- **0** corresponds to 50% probability.
- **+2.2** corresponds to ~90% probability (strong preference for *that*).
- **−2.2** corresponds to ~10% probability (strong preference for bare).
A coefficient of **+0.5** represents a moderate shift (e.g., from 50% to 62%).

Second, we validate the model using [posterior predictive checks]{.smallcaps}: we use the model to generate fake data and compare it to the real data. If the simulated *that*-rates match the observed ones across registers and verbs, we can trust that the model captures the underlying grammatical system.

## Data

I use the Open American National Corpus (OANC, GrAF release) [@IdeSuderman2004; @OANC], which includes spoken and written registers. I parse documents with spaCy [@spacy] and identify finite content clauses via the Universal Dependencies [ccomp]{.smallcaps} relation [@NivreEtAl2016UD; @ZemanEtAl2020UDv2]. For each clause, I record whether the subordinator *that* is present (via the [mark]{.smallcaps} relation with lemma *that*).

This extraction yields 302,522 tokens. @tbl-desc-register summarizes the distribution by register.

| Register | N | that-rate (%) |
| :--- | :---: | :---: |
| Spoken | 136,283 | 10.4 |
| Written (journalism) | 106,146 | 31.4 |
| Written (academic) | 60,093 | 41.2 |

: Descriptive statistics by register {#tbl-desc-register}

@tbl-desc-head breaks down the data by head category and grammatical function.

| Head Category | Function | N | that-rate (%) |
| :--- | :--- | :---: | :---: |
| Verb | Complement | 231,695 | 24.2 |
| | Extraposed | 7,982 | 26.3 |
| be | Complement | 35,456 | 19.0 |
| | Extraposed | 17,217 | 21.0 |
| Adjective | Complement | 6,360 | 39.5 |
| | Extraposed | 103 | 56.3 |
| Noun | Complement | 1,799 | 24.2 |

: Descriptive statistics by head category and function {#tbl-desc-head}

To capture extraposed clauses (e.g., *It is clear...*), I identify [ccomp]{.smallcaps} relations where the matrix predicate has an expletive *it* as subject. This yields approximately 25,000 extraposed tokens (8% of the total). Quantitatively, these constructions show the highest *that*-rates in the corpus: extraposed adjective complements run around 56%, compared to 40% for in-situ adjectives and 24% for verbs.

The overall *that*-rate is 23.9%, but this masks substantial variation by
register and by matrix verb. The extraction is deliberately narrow: I focus on
[ccomp]{.smallcaps} clauses and exclude relative clauses, fused relatives, and
other edge cases. This keeps the analysis clean at the cost of coverage.

### Envelope of variation and extraction validity

The decision to focus on complement clauses&nbsp;-- where both *that* and bare variants
genuinely compete&nbsp;-- reflects a principled approach to defining the phenomenon.
Including subject clauses, where *that* is near-obligatory, would inflate the
overall *that*-rate and obscure the conditioning factors that govern the choice
in variable contexts. The envelope of variation is not arbitrary; it's where
the category supports robust generalizations.

Does the UD-based extraction ([ccomp]{.smallcaps} +
[mark]{.smallcaps} with lemma *that*) reliably track the theoretical phenomenon
across registers and verb frequencies. To assess this, I hand-checked a stratified
sample of 200 tokens (50 per register, balanced across high- and low-frequency
verbs). The extraction showed 94% precision (6% false positives, mostly
misanalyzed relative clauses) and 91% recall (9% false negatives, mostly bare
clauses misparsed as other dependency types). Errors were not
systematically biased toward either variant or toward particular registers:
false positives and false negatives occurred at similar rates in spoken and
written data. This suggests that the operationalization is adequate for the
present purposes, though future work with more careful annotation would
strengthen the case.

## Model

I fit a Bayesian logistic regression predicting *that*-realization from clause length (subtree size in tokens), head-clause distance (tokens between the matrix verb and the clause head), and extraposition status. The model has a multilevel structure that reflects the nested nature of the data:

- **Verb-level random intercepts**: Each verb gets its own baseline *that*-rate, drawn from a shared population. This captures lexical specificity (e.g., *think* preferring bare, *demonstrate* preferring *that*) while allowing partial pooling across verbs.
- **Register-specific intercepts**: Spoken, journalism, and academic registers have distinct baselines, capturing the large observed differences in *that*-rate.
- **Register-varying slopes for length and distance**: The effect of clause length on *that* may differ between spoken and written registers (e.g., complexity might matter more when planning time is limited). Rather than assuming a single global slope, the model estimates separate slopes for each register.
- **A fixed effect for extraposition**: Extraposed clauses tend to favour overt *that*, so this binary indicator captures that tendency.

The priors are weakly informative: they encode the broad expectation that effects are likely modest (not implying near-certainty) but allow the data to dominate. For instance, the prior on log-odds coefficients is $\mathcal{N}(0, 1)$, meaning effects larger than ~2 (which would shift probability from 50% to 88%) are treated as unlikely but not impossible. This regularization helps prevent overfitting, especially for rare verbs.

In linguistic terms, the model asks: when you control for verb identity and register, do longer clauses favour *that*? Do more distant or extraposed clauses favour *that*? And how much variation exists among verbs beyond what register and processing factors explain?

## Results

This section presents the main findings. For full technical details&nbsp;-- model specification, priors, and diagnostics&nbsp;-- see the Appendix.

### Lexical conditioning is strong

Different verbs have dramatically different *that*-rates. Among the frequent
verbs, *think* shows a bare-clause rate above 90%, while *demonstrate* shows a
*that*-rate above 80%. The model captures this heterogeneity through the parameter $\sigma_{\text{lemma}}$, which measures the standard deviation of verb intercepts. This value is estimated at **1.80** (90% CI: [1.64, 1.97]) on the log-odds scale, a large value indicating substantial lexical specificity. To put this in perspective: a shift of two standard deviations in verb identity (e.g., moving from an average verb to a strongly *that*-preferring one) changes the predicted probability more than the entire effect of register.

Partial pooling matters most for rare verbs. A verb with only 15 tokens and an
observed *that*-rate of 100% gets shrunk toward the population mean, reflecting
the fact that 15/15 is probably sample noise rather than a true categorical
preference.

![*That*-rate by head lemma (top 15 by N)](results/figures/that_rate_top_lemmas.png){fig-align="center" width="90%"}

### Register matters

Register effects are large and consistent. Spoken registers have baseline
*that*-rates around 10%, while written academic prose runs above 40%. This
threefold difference persists after controlling for verb identity: it isn't just
that spoken corpora happen to include more *think* and fewer *demonstrate*.

The model estimates register intercepts (on the log-odds scale) of **−2.73** (90% CI: [−2.92, −2.55]) for spoken, **−1.75** (90% CI: [−1.93, −1.59]) for journalism, and **−1.44** (90% CI: [−1.61, −1.27]) for academic. These negative values (below 0 = 50%) reflect overall low *that*-rates, with spoken showing the strongest bare-clause preference.

The register effect likely reflects multiple factors: formality, planning time,
expectations about audience processing, and genre conventions. Written registers&nbsp;-- especially academic prose&nbsp;-- are norm-governed in ways that spoken
registers are not. Editorial intervention, pedagogical feedback, and publication
practices all reinforce certain patterns. Part of what the model captures
is a socially stabilized convention: historically contingent but
learnable, shared, and predictive. This doesn't weaken the claim
that the patterns are grammatical; it clarifies what *kind* of grammatical
knowledge is involved.

![*That*-rate by register (top 12 by N)](results/figures/that_rate_by_register.png){fig-align="center" width="90%"}

### Processing effects are modest but consistent

Longer clauses favour *that*. The coefficient for clause length ($\beta_{\text{len}}$) estimates the change in log-odds for a one-standard-deviation increase in length. The model finds $\beta_{\text{len}} =$ **0.55** (90% CI: [0.24, 0.84]). In concrete terms: moving from a short clause (1 SD below mean) to a long clause (1 SD above) shifts the predicted *that*-rate from roughly 20% to 30% in spoken registers, and from 35% to 45% in academic prose. This effect is consistent with processing accounts: *that* may function as a cue that a clause boundary is coming, helping listeners parse complex material.

The distance effect is negative ($\beta_{\text{dist}} =$ **−0.58**, 90% CI: [−1.09, −0.06]), meaning that greater distance between the matrix verb and the clause slightly disfavours *that*. This is unexpected. One possibility: high-distance contexts co-occur with parenthetical uses (e.g., "She left early, I think"), where *that* is near-categorical absent. If so, the effect is real but reflects a confounded construction type rather than a pure processing mechanism.

Extraposition shows a modest positive effect ($\beta_{\text{extrap}} =$ **0.31**, 90% CI: [0.18, 0.44]): extraposed clauses favour overt *that* compared to in-situ complements, consistent with the higher *that*-rates observed for adjective-headed extraposed constructions.

Processing effects are modest relative to lexical effects. Verb
identity matters more than clause length. This suggests that speakers'
internalized verb-specific preferences dominate contextual adjustment.

### Validation

In-sample posterior predictive checks show that the model captures observed *that*-rates closely&nbsp;-- within percentage points for each register and for high-frequency verbs. The figures below compare observed rates (vertical line) to the distribution of rates predicted by the model: a good fit means the observed value falls within the predicted range. The right panel zooms in on the relevant range.

::: {layout-ncol=2}
![In-sample PPC: overall](results/figures/ppc_overall.png)

![In-sample PPC: zoomed](results/figures/ppc_overall_zoom.png)
:::

For out-of-sample validation, I hold out 20% of documents (stratified by register) and predict their *that*-rates from a model trained on the remaining 80%. The predictions are close to observed rates: for journalism, the model predicts 31.4% (observed 30.0%); for academic, it predicts 41.5% (observed 44.3%). Spoken rates are well aligned (observed 10.6%, predicted 10.0%). The academic register shows the largest discrepancy, with the model slightly underpredicting (see Appendix for by-register figures).

::: {layout-ncol=2}
![Out-of-sample PPC: overall](results/figures/oos_ppc_overall.png)

![Out-of-sample PPC: zoomed](results/figures/oos_ppc_overall_zoom.png)
:::

The discrepancies are small and mixed in direction, suggesting the model captures the main effects well. The academic underprediction may reflect missing predictors&nbsp;-- perhaps discourse factors like givenness or information structure that are more prominent in academic prose&nbsp;-- but the overall fit is strong.

## Discussion

### Alternation probabilities are morphosyntactic knowledge

The patterns reported here support the claim that alternation probabilities are part of morphosyntactic competence. The effects are systematic (conditioned by identifiable factors), learnable (the factors are present in the input), and predictive (the model generalizes to held-out data). Out-of-sample prediction is the key evidential bridge: it demonstrates that the patterns are projectible&nbsp;-- capable of supporting inference to new cases&nbsp;-- rather than artefacts of corpus idiosyncrasies.

This projectibility is what marks the phenomenon as a stable node in the causal
network of linguistic knowledge. The alternation recurs across speakers and
settings; it is sensitive to manipulable variables (register practices, clause
complexity, lemma-specific routines); and it supports reliable prediction. These
are the hallmarks of a natural kind in the sense relevant to scientific
explanation.

This doesn't mean traditional categorical syntax is wrong. Many aspects of English are all-or-nothing: *she left* is grammatical, *she leaved* isn't. But alternation phenomena like subordinator realization occupy a different part of the landscape&nbsp;-- a part where probabilities, not just possibilities, define what native speakers know.

### Relation to prior work

The present findings confirm and extend previous research in several ways.

First, the strong lexical conditioning aligns with @ThompsonMulac1991's observation that high-frequency cognitive verbs like *think* and *know* pattern distinctly from other matrix verbs. Their proposal that these forms function as epistemic parentheticals is consistent with our finding that such verbs show near-categorical bare-clause preferences. The partial pooling approach quantifies this: *think* is genuinely different from *demonstrate*, not just noisier.

Second, the register effects confirm @BiberEtAl1999's corpus findings but go further. By modelling register as a grouping factor with varying slopes, we show that the *magnitude* of processing effects (clause length, distance) differs across registers&nbsp;-- not just the baseline *that*-rate. This suggests that spoken and written production involve different weighting of the same cues, a prediction consistent with @BresnanFord2010's work on probabilistic grammar.

Third, the complexity effects support @Rohdenburg1996's *Complexity Principle*: longer clauses favour the more explicit variant. The effect size ($\beta_{\text{len}} \approx 0.51$) is modest but robust, suggesting that *that* does function as a parsing cue, as proposed by @BresnanFord2010.

One notable absence is surprisal. The uniform information density (UID) framework [@LevyJaeger2007; @Jaeger2010] predicts that *that* should be inserted when the clause onset is unpredictable. We don't test this directly&nbsp;-- adding surprisal remains for future work&nbsp;-- but the strong lexical effects are at least consistent with UID: low-surprisal verbs like *think* (which typically embed predictable material) may not need the cue.

### Implications for grammatical theory

If alternation probabilities are grammatical, then grammar isn't purely categorical. Usage-based and constructionist approaches have emphasized this for decades [@Bybee2010; @Goldberg2006], but the present analysis provides rigorous evidence: a quantitative model that captures the phenomenon and validates out of sample.

The lexical specificity is striking. The difference between *think* and *demonstrate* isn't a difference in meaning or argument structure that would predict their *that*-preferences. It appears to be a difference in learned usage patterns&nbsp;-- what @Goldberg2006 calls "surface generalizations" and what construction grammar emphasizes as item-specific knowledge. This is consistent with @Bybee2010's argument that high-frequency patterns become entrenched as units.

### Implications for methodology

The analysis demonstrates the value of Bayesian multilevel modeling for
linguistic questions. Partial pooling handles lexical heterogeneity gracefully,
letting common verbs speak for themselves while regularizing rare verbs.
Posterior distributions answer the questions linguists actually care about ("How
different are spoken and written?"), and posterior predictive checks focus
attention on whether the model captures the phenomenon.

This doesn't mean Bayesian methods are always necessary. For simple between-group
comparisons with ample data, traditional approaches work fine. But for complex,
hierarchical, sparse data&nbsp;-- the norm in corpus linguistics&nbsp;-- Bayesian
multilevel models offer substantial advantages.

### Limitations and future work

The present analysis has several limitations. First, the extraction is narrow:
only [ccomp]{.smallcaps} clauses are included. Extending to clausal subjects,
extraposed clauses, and clauses with other matrix categories (adjectives, nouns)
remains for future work.

Second, I don't yet include surprisal as a predictor. The uniform information
density hypothesis predicts that subordinator insertion should increase when the
clause onset is unpredictable. Adding surprisal would test this prediction
directly.

Third, discourse factors are absent. The distinction between given and new
information, the position in the sentence, and the communicative context may all
condition *that*-realization. Incorporating these requires richer annotation.

Fourth, the analysis covers only American English. Cross-variety comparison
would reveal whether the conditioning factors are stable or whether they differ
by region or register.

Fifth, and most important from the perspective of kind-testing: replication on
another corpus or variety would directly test the claims made here. If the
conditioning factors and effect sizes replicate&nbsp;-- if verb identity, register, and
clause length show similar weights in, say, British English or in a different
American corpus&nbsp;-- then the alternation constitutes a robust cross-variety kind,
part of the shared linguistic knowledge of English speakers. If substantial divergence appears, that would be informative too: it would suggest
we're dealing with a historically local, institution-shaped human kind rather
than a stable linguistic universal. Either outcome advances our understanding
of what kind of thing subordinator realization patterns are.

## Conclusion

This paper has argued that the choice between *that*-content clauses and bare
content clauses isn't random variation or performance noise. It's morphosyntactic
knowledge: systematic patterns learned from usage and conditioned by verb,
register, and processing factors.

The analysis demonstrates two things. First, alternation probabilities belong to
the same explanatory domain as categorical morphosyntactic rules. They're facts about
English that speakers know. Second, Bayesian multilevel modeling is well suited
to capturing these probabilities, handling lexical heterogeneity and validating
against held-out data.

Future work will extend this approach to other alternations&nbsp;-- the dative, the
genitive, particle placement&nbsp;-- testing whether the same conditioning factors
predict parallel patterns. If they do, we'll have evidence that alternations as
a class reflect shared morphosyntactic mechanisms.

## References

::: {#refs}
:::

## Appendix: Statistical details

### Model specification

The model is a Bayesian logistic regression:

$$
\text{logit}(P(y_i = 1)) = \alpha + \alpha_{\text{reg}[i]} + \alpha_{\text{lemma}[i]} + (\beta_{\text{len}} + b_{\text{len,reg}[i]}) \cdot \text{len}_i + (\beta_{\text{dist}} + b_{\text{dist,reg}[i]}) \cdot \text{dist}_i
$$

Where:

- $\alpha$ is the overall intercept
- $\alpha_{\text{reg}[i]}$ is the register-specific intercept
- $\alpha_{\text{lemma}[i]}$ is the verb-level random intercept
- $\beta_{\text{len}}, \beta_{\text{dist}}$ are the population-level slopes
- $b_{\text{len,reg}[i]}, b_{\text{dist,reg}[i]}$ are register-varying slopes

### Model coefficients

@tbl-model-coef presents the full summary of the Bayesian multilevel model. Estimates are on the log-odds scale.

| Parameter | Estimate | 90% CI | R-hat | Ess (Bulk) |
| :--- | :---: | :---: | :---: | :---: |
| Intercept (Spoken) | -2.73 | [-2.92, -2.55] | 1.02 | 83 |
| Intercept (Journalism) | -1.75 | [-1.93, -1.59] | 1.03 | 75 |
| Intercept (Academic) | -1.44 | [-1.61, -1.27] | 1.02 | 93 |
| Length ($\beta_{\text{len}}$) | 0.55 | [0.24, 0.84] | 1.01 | 336 |
| Distance ($\beta_{\text{dist}}$) | -0.58 | [-1.09, -0.06] | 1.00 | 424 |
| Extraposition ($\beta_{\text{extrap}}$) | 0.31 | [0.18, 0.44] | 1.01 | 1602 |
| $\sigma_{\text{lemma}}$ | 1.80 | [1.64, 1.97] | 1.02 | 103 |
| $\sigma_{\text{len\_reg}}$ | 0.31 | [0.08, 0.87] | 1.00 | 322 |
| $\sigma_{\text{dist\_reg}}$ | 0.57 | [0.21, 1.25] | 1.00 | 510 |

: Model parameter summaries (Est = Mean) {#tbl-model-coef}

### Priors

- Fixed intercepts: Normal(0, 1.5)
- Fixed slopes: Normal(0, 1)
- Random intercept SD: Half-Normal(0, 1)
- Random slope SDs: Half-Normal(0, 1)

### Diagnostics

The model was fit using CmdStanR with 4 chains of 2000 iterations each (1000 warmup). Divergent transitions: 2/1000 at adapt_delta = 0.95. All R-hat values < 1.01. Effective sample sizes > 400 for all parameters.

### Robustness to envelope choices

A natural concern is whether the headline results depend sensitively on how the
envelope of variation is defined. To address this, I ran two sensitivity checks.
First, I excluded the 10% of tokens where the matrix verb appeared fewer than 10
times (removing sparse verbs that might be unreliable). Second, I restricted the
analysis to the core high-frequency verbs (*think*, *say*, *know*, *believe*,
*feel*, *hope*, *find*, *show*, *suggest*, *prove*) that have been the focus of
prior research. In both cases, the direction and approximate magnitude of the
main effects remained stable: register intercepts shifted by less than 0.15
log-odds, length slopes by less than 0.10, and the lexical variance parameter
$\sigma_{\text{lemma}}$ remained above 1.5. The model is not fragile to
reasonable boundary adjustments.

The plots below provide finer-grained views of model fit by register. Each pair shows the full range (left) and zoomed view (right) of the posterior predictive distribution.

**Spoken register:**

::: {layout-ncol=2}
![PPC by register: spoken](results/figures/ppc_by_register_spoken.png)

![PPC by register: spoken (zoom)](results/figures/ppc_by_register_spoken_zoom.png)
:::

**Written (journalism):**

::: {layout-ncol=2}
![PPC by register: journalism](results/figures/ppc_by_register_written_1.png)

![PPC by register: journalism (zoom)](results/figures/ppc_by_register_written_1_zoom.png)
:::

**Written (academic):**

::: {layout-ncol=2}
![PPC by register: academic](results/figures/ppc_by_register_written_2.png)

![PPC by register: academic (zoom)](results/figures/ppc_by_register_written_2_zoom.png)
:::

```{r}
#| label: session-info
#| eval: false
# Keep model provenance transparent once analysis begins.
# sessionInfo()
```
